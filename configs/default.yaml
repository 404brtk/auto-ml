# AutoML Pipeline Configuration
# This YAML file contains all configuration options for the AutoML pipeline.
# Comment/uncomment and modify values as needed.

# ML task type (auto-detected if not specified)
# task: classification  # or "regression"

# Data splitting configuration
split:
  test_size: 0.2          # Proportion of data for testing (0.1-0.9)
  random_state: 42        # Random seed for reproducibility
  stratify: true          # Stratify split based on target (recommended)
  n_splits: 5             # Number of cross-validation folds (2-20)

# Data cleaning configuration
cleaning:
  drop_duplicates: true           # Remove duplicate rows
  max_missing_row_ratio: 0.5      # Max missing ratio per row before dropping (0-1)
  remove_constant_features: true  # Remove constant/quasi-constant features
  constant_tolerance: 1.0         # Threshold for constant features (1.0=100% constant, 0.95=95%+ same)
  remove_id_columns: true         # Remove columns with >95% unique values (likely IDs)
  id_column_threshold: 0.95       # Uniqueness ratio threshold for ID column detection
  handle_mixed_types: "coerce"    # How to handle mixed type columns: "coerce" or "drop"
  numeric_coercion_threshold: 0.9 # Minimum ratio of values that must be numeric-coercible for object columns (0-1)
  uniqueness_ratio_threshold: 0.5 # Threshold for uniqueness ratio in integer targets to classify as classification vs regression (0-1)
  min_rows_after_cleaning: 1      # Minimum rows required after cleaning (raises error if below)
  min_cols_after_cleaning: 1      # Minimum columns required after cleaning (raises error if below)
  special_null_values:            # List of string values to treat as null/missing values
    - "?"
    - "N/A"
    - "n/a"
    - "NA"
    - "null"
    - "NULL"
    - "None"
    - "none"
    - "nan"
    - "NaN"
    - "NAN"
    - "undefined"
    - "missing"
    - "MISSING"
    - "-"
    - "--"
    - "---"
    - ""
    - " "

  # Outlier detection (fitted on training data, applied to test if method=clip)
  outlier:
    strategy: null        # "iqr", "zscore", or null to disable
    method: "clip"        # "clip" or "remove" detected outliers
    iqr_multiplier: 1.5   # IQR multiplier (1.5=standard, 3.0=conservative)
    zscore_threshold: 3.0 # Z-score threshold for outlier detection

# Feature engineering configuration
features:
  # Missing value imputation
  imputation:
    strategy_cat: "most_frequent" # "most_frequent" or "random_sample" for categorical features
    strategy_num: "median"        # "mean", "median", "knn", or "random_sample" for numeric features
    knn_neighbors: 5              # Number of neighbors for KNN imputation (1-20)
    random_sample_seed: 42        # Random seed for random_sample imputation

  # Feature scaling
  scaling:
    strategy: "standard"  # "standard", "minmax", "robust", or "none"

  # Categorical encoding
  encoding:
    high_cardinality_number_threshold: 100  # Number of unique values to consider a feature as high cardinality (2-1000)
    high_cardinality_pct_threshold: 0.1     # Percentage of unique values to consider a feature as high cardinality (0-1)
    high_cardinality_encoder: "target"      # Encoder to use for high cardinality features: "target" or "frequency"
    scale_high_card: false                  # Apply scaling to high cardinality encoded features

  # Feature extraction
  extract_datetime: true   # Extract features from datetime columns
  extract_time: true       # Extract features from time-only columns
  handle_text: false       # Enable text feature extraction (resource intensive)
  max_features_text: 2000  # Maximum text features to extract (100-10000)
  text_length_threshold: 50  # Minimum text length for feature extraction (10-500)

# Feature selection configuration
selection:
  # Variance-based selection
  variance_threshold: null  # Remove features with variance below threshold (0+)

  # Correlation-based selection
  correlation_threshold: null  # Remove features with correlation above threshold (0-1)
  correlation_method: "pearson" # Correlation method: "pearson", "spearman", or "kendall"

  # Univariate selection using mutual information
  mutual_info_k: null       # Select top K features using mutual information (1+)

  # Dimensionality reduction with PCA
  pca_components: null      # Number of components (int) or variance ratio (0-1)

# Hyperparameter optimization configuration
optimization:
  enabled: true            # Enable hyperparameter optimization
  n_trials: 50             # Number of optimization trials (1-1000)
  timeout: 3600            # Timeout in seconds (60+), null for no limit
  retrain_on_full_data: true  # Retrain best model on full dataset for production
  sampler: "tpe"           # Sampler algorithm: "tpe" (Tree-structured Parzen Estimator), "random", or "cmaes"
  pruner: "median"         # Pruner for early stopping: "median", "successive_halving", "hyperband", or "none"
  pruner_startup_trials: 5 # Number of trials before pruning starts (0-20)

# Model evaluation configuration
eval:
  metrics: null            # Custom metrics list, null uses defaults

# Input/Output configuration
io:
  dataset_path: null       # Path to input dataset (set via CLI)
  target: null             # Target column name (set via CLI)
  output_dir: "outputs"    # Directory for output files

# Model selection configuration
models:
  include: ["xgboost"]      # Specific models to include, null = all available
  exclude: null             # Models to exclude from training
